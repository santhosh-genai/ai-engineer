import express from 'express';
import cors from 'cors';
import multer from 'multer';
import path from 'path';
import fs from 'fs';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import dotenv from 'dotenv';
import { spawn } from 'child_process';
import { MongoClient } from 'mongodb';
import dns from 'dns';
import axios from 'axios';
import { generateEmbedding, generateBatchEmbeddings } from '../src/scripts/utilities/mistralEmbedding.js';
import { rerankDocuments, summarizeResults } from '../src/scripts/utilities/groqClient.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

dotenv.config();

// Fix DNS resolution issue on macOS
dns.setServers(['8.8.8.8', '8.8.4.4']);

const app = express();
const PORT = process.env.PORT || 3001;

// Middleware
app.use(cors());
app.use(express.json());
app.use('/uploads', express.static(path.join(__dirname, '../uploads')));

// Configure multer for file uploads
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    const uploadPath = path.join(__dirname, '../uploads');
    if (!fs.existsSync(uploadPath)) {
      fs.mkdirSync(uploadPath, { recursive: true });
    }
    cb(null, uploadPath);
  },
  filename: (req, file, cb) => {
    cb(null, `${Date.now()}-${file.originalname}`);
  }
});

const upload = multer({ storage });

// ======================== Job Tracking ========================
// In-memory job tracking (consider using Redis for production)
const jobs = new Map();

function createJob(files) {
  const jobId = `job-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
  jobs.set(jobId, {
    id: jobId,
    files,
    status: 'in-progress',
    progress: 0,
    total: files.length,
    results: [],
    startTime: new Date(),
    currentFile: null
  });
  return jobId;
}

function updateJob(jobId, updates) {
  const job = jobs.get(jobId);
  if (job) {
    Object.assign(job, updates);
    jobs.set(jobId, job);
  }
}

function getJob(jobId) {
  return jobs.get(jobId);
}

// Clean up old jobs (older than 1 hour)
setInterval(() => {
  const oneHourAgo = Date.now() - 60 * 60 * 1000;
  for (const [jobId, job] of jobs.entries()) {
    if (new Date(job.startTime).getTime() < oneHourAgo) {
      jobs.delete(jobId);
    }
  }
}, 10 * 60 * 1000); // Run every 10 minutes

// ======================== Validation Helpers ========================

async function validateDbCollectionIndex(client, dbName, collectionName, indexName, requireDocuments = false) {
  try {
    // Attempt to detect database existence via listDatabases (may require privileges)
    let dbExists = false;
    try {
      const admin = client.db().admin();
      const dbs = await admin.listDatabases();
      dbExists = dbs.databases.some(d => d.name === dbName);
    } catch (err) {
      // If listDatabases fails because of permissions, fallback to checking the collection directly
      console.warn('âš ï¸ listDatabases failed (permissions?), falling back to listCollections check:', err.message);
      dbExists = true; // assume DB exists and proceed to collection check
    }

    if (!dbExists) {
      return { ok: false, error: `Database '${dbName}' not found` };
    }

    const db = client.db(dbName);
    const collections = await db.listCollections({ name: collectionName }).toArray();
    if (!collections || collections.length === 0) {
      return { ok: false, error: `Collection '${collectionName}' not found in database '${dbName}'` };
    }

    if (requireDocuments) {
      const count = await db.collection(collectionName).countDocuments();
      if (count === 0) {
        return { ok: false, error: `No documents found in collection '${collectionName}'. Please create embeddings first.` };
      }
    }

    // Verify Atlas Search indexes (listSearchIndexes command)
    if (indexName) {
      try {
        const collection = db.collection(collectionName);
        const indexes = await collection.listSearchIndexes().toArray();
        if (!indexes || !Array.isArray(indexes)) {
          return { ok: false, error: `Unable to verify search indexes for collection '${collectionName}'.` };
        }
        const found = indexes.some(idx => idx.name === indexName);
        if (!found) {
          return { ok: false, error: `Search index '${indexName}' not found for collection '${collectionName}'` };
        }
      } catch (err) {
        // Some server versions / permissions may not allow listSearchIndexes; surface helpful message
        return { ok: false, error: `Could not verify search index '${indexName}': ${err.message}` };
      }
    }

    return { ok: true };

  } catch (err) {
    return { ok: false, error: `Validation failed: ${err.message}` };
  }
}

// ======================== API Routes ========================

// Health check
app.get('/api/health', (req, res) => {
  res.json({ status: 'OK', message: 'Server is running' });
});

// Get active jobs
app.get('/api/jobs/active', (req, res) => {
  const activeJobs = Array.from(jobs.values()).filter(job => job.status === 'in-progress');
  res.json({ jobs: activeJobs });
});

// Get job status
app.get('/api/jobs/:jobId', (req, res) => {
  const job = getJob(req.params.jobId);
  if (!job) {
    return res.status(404).json({ error: 'Job not found' });
  }
  res.json(job);
});

// Get distinct metadata values for filters
app.get('/api/metadata/distinct', async (req, res) => {
  try {
    console.log('ðŸ” Fetching distinct metadata values...');
    console.log('ðŸ“Š DB Name:', process.env.DB_NAME);
    console.log('ðŸ“Š Collection Name:', process.env.COLLECTION_NAME);
    
    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });

    await mongoClient.connect();
    console.log('âœ… Connected to MongoDB');
    
    const db = mongoClient.db(process.env.DB_NAME);
    const collection = db.collection(process.env.COLLECTION_NAME);

    // Check document count first
    const count = await collection.countDocuments();
    console.log(`ðŸ“Š Total documents in collection: ${count}`);

    if (count === 0) {
      console.log('âš ï¸ Collection is empty! No documents found.');
      await mongoClient.close();
      return res.json({
        success: true,
        metadata: {
          modules: [],
          priorities: [],
          risks: [],
          types: []
        },
        message: 'Collection is empty. Please create embeddings first.'
      });
    }

    // Get a sample document to see what fields exist
    const sampleDoc = await collection.findOne({});
    console.log('ðŸ“„ Sample document fields:', Object.keys(sampleDoc || {}));
    console.log('ðŸ“„ Sample document:', JSON.stringify(sampleDoc, null, 2));

    const modules = await collection.distinct('module');
    const priorities = await collection.distinct('priority');
    const risks = await collection.distinct('risk');
    const types = await collection.distinct('automationManual');

    console.log(`âœ… Found ${modules.length} modules:`, modules);
    console.log(`âœ… Found ${priorities.length} priorities:`, priorities);
    console.log(`âœ… Found ${risks.length} risks:`, risks);
    console.log(`âœ… Found ${types.length} types:`, types);

  await mongoClient.close();

    const metadata = {
      modules: modules.filter(Boolean).sort(),
      priorities: priorities.filter(Boolean).sort(),
      risks: risks.filter(Boolean).sort(),
      types: types.filter(Boolean).sort()
    };

    console.log('ðŸ“¤ Sending metadata:', metadata);

    res.json({
      success: true,
      metadata
    });

  } catch (error) {
    console.error('âŒ Error fetching metadata:', error);
    res.status(500).json({ error: 'Failed to fetch metadata', details: error.message });
  }
});

// Get all files in data directory
app.get('/api/files', (req, res) => {
  try {
    const dataPath = path.join(__dirname, '../src/data');
    const files = fs.readdirSync(dataPath)
      .filter(file => file.endsWith('.json'))
      .map(file => {
        const filePath = path.join(dataPath, file);
        const stats = fs.statSync(filePath);
        return {
          name: file,
          path: filePath,
          size: stats.size,
          modified: stats.mtime,
          type: 'json'
        };
      });
    res.json(files);
  } catch (error) {
    res.status(500).json({ error: 'Failed to read files', details: error.message });
  }
});

// Upload and convert Excel to JSON
app.post('/api/upload-excel', upload.single('file'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    const inputFile = req.file.path;
    const outputPath = path.join(__dirname, '../src/data', `converted-${Date.now()}.json`);
    
    // Convert paths to forward slashes (works on both Windows and Unix)
    const inputFileNormalized = inputFile.replace(/\\/g, '/');
    const outputPathNormalized = outputPath.replace(/\\/g, '/');
    
    // Create a modified version of excel-to-json.js for this specific file
    const scriptContent = `
import xlsx from "xlsx";
import fs from "fs";

const excelFile = "${inputFileNormalized}";      
const sheetName = "${req.body.sheetName || 'Testcases'}";   
const outputFile = "${outputPathNormalized}";      

const columnMap = {
  "Module": "module",
  "Test ID": "id",
  "Pre-Requisites": "preRequisites",
  "Test Title": "title",
  "Test Case Description": "description",
  "Test Steps": "steps",
  "Expected Results": "expectedResults",
  "Automation/Manual": "automationManual",
  "Priority": "priority",
  "Created By": "createdBy",
  "Created Date": "createdDate",
  "Last modified date": "lastModifiedDate",
  "Risk": "risk",
  "Version": "version",
  "Type": "type"
};

try {
  const workbook = xlsx.readFile(excelFile);
  const worksheet = workbook.Sheets[sheetName];

  if (!worksheet) {
    console.error(\`âŒ Sheet "\${sheetName}" not found in \${excelFile}\`);
    process.exit(1);
  }

  const rawData = xlsx.utils.sheet_to_json(worksheet, { defval: "" });

  const jsonData = rawData.map((row, index) => {
    const mappedRow = {};
    for (const [excelCol, jsonKey] of Object.entries(columnMap)) {
      mappedRow[jsonKey] = row[excelCol] || "";
    }
    return mappedRow;
  });

  fs.writeFileSync(outputFile, JSON.stringify(jsonData, null, 2), "utf-8");
  console.log(\`âœ… Converted \${jsonData.length} rows from "\${sheetName}" into \${outputFile}\`);
} catch (error) {
  console.error('âŒ Error:', error.message);
  process.exit(1);
}
`;

    const tempScriptPath = path.join(__dirname, `temp-excel-convert-${Date.now()}.js`);
    fs.writeFileSync(tempScriptPath, scriptContent);

    // Execute the conversion script
    const child = spawn('node', [tempScriptPath], { cwd: __dirname });
    
    let output = '';
    let error = '';

    child.stdout.on('data', (data) => {
      output += data.toString();
    });

    child.stderr.on('data', (data) => {
      error += data.toString();
    });

    child.on('close', (code) => {
      // Clean up temp script and uploaded file
      fs.unlinkSync(tempScriptPath);
      fs.unlinkSync(inputFile);

      if (code === 0) {
        res.json({
          success: true,
          message: 'File converted successfully',
          outputFile: path.basename(outputPath),
          output
        });
      } else {
        res.status(500).json({
          success: false,
          error: 'Conversion failed',
          details: error || output
        });
      }
    });

  } catch (error) {
    res.status(500).json({ error: 'Upload failed', details: error.message });
  }
});

// Create embeddings for selected files
app.post('/api/create-embeddings', async (req, res) => {
  try {
    const { files } = req.body;
    
    if (!files || files.length === 0) {
      return res.status(400).json({ error: 'No files selected' });
    }

    // Validate DB and collection exist (no documents required for creating embeddings)
    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });

    try {
      await mongoClient.connect();
      const validation = await validateDbCollectionIndex(mongoClient, process.env.DB_NAME, process.env.COLLECTION_NAME, null, false);
      if (!validation.ok) {
        await mongoClient.close();
        return res.status(400).json({ error: validation.error });
      }
    } catch (err) {
      return res.status(500).json({ error: 'Failed to validate database/collection', details: err.message });
    } finally {
      try { await mongoClient.close(); } catch (e) {}
    }

    // Create a job and return immediately
    const jobId = createJob(files);
    
    // Start processing in background
    processEmbeddings(jobId, files);
    
    // Return job ID to client
    res.json({
      success: true,
      jobId,
      message: 'Embedding creation started',
      filesCount: files.length
    });

  } catch (error) {
    res.status(500).json({ error: 'Embedding creation failed', details: error.message });
  }
});

// Background processing function
async function processEmbeddings(jobId, files) {
  const results = [];

  
  for (const fileName of files) {
    updateJob(jobId, { currentFile: fileName });
    
    const filePath = path.join(__dirname, '../src/data', fileName);
    // Convert paths to forward slashes for cross-platform compatibility
    const filePathNormalized = filePath.replace(/\\/g, '/');
    
    // Create a modified version of create-embeddings script using Mistral AI
    const scriptContent = `
import { MongoClient } from "mongodb";
import dns from "dns";
import dotenv from "dotenv";
import fs from "fs";
import { generateEmbedding } from "../src/scripts/utilities/mistralEmbedding.js";

dotenv.config();

dns.setServers(['8.8.8.8', '8.8.4.4']);

const mongoClient = new MongoClient(process.env.MONGODB_URI, {
  ssl: true,
  tlsAllowInvalidCertificates: true,
  tlsAllowInvalidHostnames: true,
  serverSelectionTimeoutMS: 30000,
  connectTimeoutMS: 30000,
  socketTimeoutMS: 30000,
});

async function main() {
  try {
  await mongoClient.connect();
  const db = mongoClient.db(process.env.DB_NAME);
  const collection = db.collection(process.env.COLLECTION_NAME);

    const testcases = JSON.parse(fs.readFileSync("${filePathNormalized}", "utf-8"));

    console.log(\`ðŸš€ Processing \${testcases.length} test cases from ${fileName}...\`);
    
    let totalCost = 0;
    let totalTokens = 0;
    let processed = 0;

    for (const testcase of testcases) {
      try {
        const inputText = \`
          Module: \${testcase.module}
          ID: \${testcase.id}
          Pre-Requisites: \${testcase.preRequisites}
          Title: \${testcase.title}
          Description: \${testcase.description}
          Steps: \${testcase.steps}
          Expected Result: \${testcase.expectedResults}
          Automation/Manual: \${testcase.automationManual}
          Priority: \${testcase.priority}
          Created By: \${testcase.createdBy}
          Created Date: \${testcase.createdDate}
          Last Modified Date: \${testcase.lastModifiedDate}
          Risk: \${testcase.risk}
          Version: \${testcase.version}
          Type: \${testcase.type}
        \`;
        
        // Use Mistral AI for embeddings
        const embeddingResult = await generateEmbedding(inputText);
        
        if (!embeddingResult || !embeddingResult.embedding) {
          throw new Error('Failed to generate embedding with Mistral AI');
        }

        const vector = embeddingResult.embedding;
        const tokens = embeddingResult.usage?.total_tokens || 0;
        const cost = (tokens / 1000000) * 0.10; // Mistral pricing: $0.10 per 1M tokens
        
        totalCost += cost;
        totalTokens += tokens;

        const doc = {
          ...testcase,
          embedding: vector,
          createdAt: new Date(),
          sourceFile: "${fileName}",
          embeddingMetadata: {
            model: 'mistral-embed',
            provider: 'mistral-ai',
            cost: cost,
            tokens: tokens,
            apiSource: 'testleaf'
          }
        };

        await collection.insertOne(doc);
        processed++;
        
        console.log(\`âœ… Processed \${processed}/\${testcases.length}: \${testcase.id}\`);
        
        await new Promise(resolve => setTimeout(resolve, 100));
        
      } catch (error) {
        console.error(\`âŒ Error processing \${testcase.id}: \${error.message}\`);
        continue;
      }
    }

    console.log(\`\\nðŸŽ‰ Processing complete for ${fileName}!\`);
    console.log(\`ðŸ’° Total Cost: $\${totalCost.toFixed(6)}\`);
    console.log(\`ðŸ”¢ Total Tokens: \${totalTokens}\`);
    console.log(\`ðŸ“Š Processed: \${processed}/\${testcases.length}\`);

  } catch (err) {
    console.error("âŒ Error:", err.message);
    process.exit(1);
  } finally {
    await mongoClient.close();
  }
}

main();
`;

      const tempScriptPath = path.join(__dirname, `temp-embeddings-${Date.now()}.js`);
      fs.writeFileSync(tempScriptPath, scriptContent);

    try {
      await new Promise((resolve, reject) => {
        const child = spawn('node', [tempScriptPath], { cwd: __dirname });
        
        let output = '';
        let error = '';

        child.stdout.on('data', (data) => {
          output += data.toString();
        });

        child.stderr.on('data', (data) => {
          error += data.toString();
        });

        child.on('close', (code) => {
          fs.unlinkSync(tempScriptPath);
          
          if (code === 0) {
            results.push({
              file: fileName,
              status: 'completed',
              output
            });
            resolve();
          } else {
            results.push({
              file: fileName,
              status: 'failed',
              error: error || output
            });
            resolve(); // Continue with other files
          }
        });
      });
    } catch (error) {
      results.push({
        file: fileName,
        status: 'failed',
        error: error.message
      });
    }
    
    // Update job progress
    updateJob(jobId, {
      progress: results.length,
      results: [...results]
    });
  }

  // Mark job as complete
  updateJob(jobId, {
    status: 'completed',
    endTime: new Date(),
    results
  });
}

// Get environment variables
app.get('/api/env', (req, res) => {
  try {
    const envPath = path.join(__dirname, '../.env');
    const envContent = fs.readFileSync(envPath, 'utf-8');
    
    const envVars = {};
    envContent.split('\n').forEach(line => {
      const [key, ...valueParts] = line.split('=');
      if (key && key.trim() && !key.startsWith('#')) {
        envVars[key.trim()] = valueParts.join('=').replace(/"/g, '');
      }
    });

    res.json(envVars);
  } catch (error) {
    res.status(500).json({ error: 'Failed to read environment variables', details: error.message });
  }
});

// Update environment variables
app.post('/api/env', (req, res) => {
  try {
    const { envVars } = req.body;
    const envPath = path.join(__dirname, '../.env');
    
    let envContent = '';
    Object.entries(envVars).forEach(([key, value]) => {
      envContent += `${key}="${value}"\n`;
    });

    fs.writeFileSync(envPath, envContent);
    
    res.json({ success: true, message: 'Environment variables updated successfully' });
  } catch (error) {
    res.status(500).json({ error: 'Failed to update environment variables', details: error.message });
  }
});

// ======================== Query Preprocessing ========================
// Preprocess query: normalization, abbreviation expansion, synonym expansion
app.post('/api/search/preprocess', async (req, res) => {
  try {
    const { query, options = {} } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    // Import preprocessing modules dynamically
    const { preprocessQuery } = await import('../src/scripts/query-preprocessing/queryPreprocessor.js');

    // Preprocess the query
    const result = preprocessQuery(query, {
      enableAbbreviations: options.enableAbbreviations !== false,
      enableSynonyms: options.enableSynonyms !== false,
      maxSynonymVariations: options.maxSynonymVariations || 5,
      customAbbreviations: options.customAbbreviations || {},
      customSynonyms: options.customSynonyms || {},
      smartExpansion: options.smartExpansion || false,
      preserveTestCaseIds: options.preserveTestCaseIds !== false
    });

    res.json(result);
  } catch (error) {
    console.error('Preprocessing error:', error);
    res.status(500).json({ 
      error: 'Failed to preprocess query', 
      details: error.message 
    });
  }
});

// Analyze query (show what preprocessing would do without applying)
app.post('/api/search/analyze', async (req, res) => {
  try {
    const { query } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    const { analyzeQuery } = await import('../src/scripts/query-preprocessing/queryPreprocessor.js');
    const analysis = analyzeQuery(query);

    res.json(analysis);
  } catch (error) {
    console.error('Analysis error:', error);
    res.status(500).json({ 
      error: 'Failed to analyze query', 
      details: error.message 
    });
  }
});

// ======================== Summarization & Deduplication ========================

// Deduplicate results based on similarity
app.post('/api/search/deduplicate', async (req, res) => {
  try {
    const { results, threshold = 0.85 } = req.body;
    
    if (!results || !Array.isArray(results)) {
      return res.status(400).json({ error: 'Results array is required' });
    }

    const deduplicated = [];
    const duplicates = [];
    const seenTitles = new Map();

    for (const result of results) {
      const title = result.title?.toLowerCase() || '';
      const id = result.id || '';

      // Check for exact title match
      let isDuplicate = false;
      
      for (const [seenTitle, seenResult] of seenTitles.entries()) {
        // Calculate similarity (Jaccard similarity for simple implementation)
        const similarity = calculateTextSimilarity(title, seenTitle);
        
        if (similarity >= threshold) {
          isDuplicate = true;
          duplicates.push({
            ...result,
            duplicateOf: seenResult.id,
            similarity: similarity.toFixed(3)
          });
          break;
        }
      }

      if (!isDuplicate) {
        deduplicated.push(result);
        seenTitles.set(title, result);
      }
    }

    res.json({
      original: results,
      deduplicated,
      duplicates,
      stats: {
        originalCount: results.length,
        deduplicatedCount: deduplicated.length,
        duplicatesRemoved: duplicates.length,
        reductionPercentage: ((duplicates.length / results.length) * 100).toFixed(1)
      }
    });
  } catch (error) {
    console.error('Deduplication error:', error);
    res.status(500).json({ 
      error: 'Failed to deduplicate results', 
      details: error.message 
    });
  }
});

// Summarize search results using Groq AI
app.post('/api/search/summarize', async (req, res) => {
  try {
    const { results, summaryType = 'concise', query = '' } = req.body;
    
    if (!results || !Array.isArray(results)) {
      return res.status(400).json({ error: 'Results array is required' });
    }

    if (results.length === 0) {
      return res.json({
        success: true,
        summary: 'No results to summarize',
        summaryType,
        resultCount: 0,
        timestamp: new Date().toISOString()
      });
    }

    // Prepare comprehensive content for summarization
    // Handle both field name formats and include ALL available information
    const resultsText = results.map((r, idx) => {
      const id = r.testCaseId || r.id || 'N/A';
      const title = r.testCaseTitle || r.title || 'No title';
      const description = r.testCaseDescription || r.description || 'No description';
      const steps = r.testSteps || r.steps || [];
      const expectedResults = r.expectedResults || r.expectedResults || 'Not specified';
      const module = r.module || 'Unknown';
      const priority = r.priority || 'Not specified';
      const automationManual = r.automationManual || r.automationStatus || 'Not specified';
      const risk = r.risk || 'Not specified';
      const type = r.type || 'Functional';
      
      let testCaseDetail = `${idx + 1}. TEST CASE: ${id}`;
      testCaseDetail += `\n   MODULE: ${module}`;
      testCaseDetail += `\n   PRIORITY: ${priority} | RISK: ${risk} | TYPE: ${type} | AUTOMATION: ${automationManual}`;
      testCaseDetail += `\n   TITLE: ${title}`;
      testCaseDetail += `\n   DESCRIPTION: ${description}`;
      
      if (steps) {
        testCaseDetail += `\n   TEST STEPS:`;
        if (Array.isArray(steps) && steps.length > 0) {
          steps.forEach((step, stepIdx) => {
            testCaseDetail += `\n     ${stepIdx + 1}. ${step}`;
          });
        } else if (typeof steps === 'string' && steps.trim()) {
          // Handle case where steps is a string (split by common delimiters)
          const stepArray = steps.split(/\r?\n|\r/).filter(step => step.trim());
          stepArray.forEach((step, stepIdx) => {
            testCaseDetail += `\n     ${stepIdx + 1}. ${step.trim()}`;
          });
        } else {
          testCaseDetail += `\n     ${steps}`;
        }
      }
      
      testCaseDetail += `\n   EXPECTED RESULTS: ${expectedResults}`;
      testCaseDetail += `\n   ----------------------------------------`;
      
      return testCaseDetail;
    }).join('\n\n');

    const systemPrompt = summaryType === 'detailed'
      ? `You are a senior QA expert specializing in healthcare systems with 10+ years of experience. 

Your task is to provide a COMPREHENSIVE analysis of the test cases including:

1. **FUNCTIONAL COVERAGE ANALYSIS**: Group test cases by modules/functionality
2. **PRIORITY & RISK ASSESSMENT**: Analyze priority distribution and risk coverage
3. **TEST SCENARIO DEPTH**: Evaluate completeness of test steps and expected results
4. **EDGE CASES & NEGATIVE SCENARIOS**: Identify what edge cases are covered
5. **AUTOMATION READINESS**: Assess automation vs manual distribution
6. **CRITICAL GAPS**: Identify missing test scenarios that should exist
7. **HEALTHCARE COMPLIANCE**: Note any regulatory/compliance testing gaps
8. **INTEGRATION POINTS**: Identify inter-module dependencies that need testing

Provide detailed insights with specific examples from the test cases. Be thorough and technical.`
      : 'You are a QA expert specializing in healthcare systems. Provide a concise summary of the test cases in 2-3 sentences, highlighting the main functionality being tested and key scenarios covered.';

    const userPrompt = summaryType === 'detailed' 
      ? `Analyze the following healthcare test cases in detail. Provide comprehensive coverage analysis:\n\n${resultsText}`
      : `Summarize the following test cases:\n\n${resultsText}`;

    // Use Groq AI for summarization
    console.log('ðŸ¤– Using Groq AI for summarization...');
    
    if (!process.env.GROQ_API_KEY || process.env.GROQ_API_KEY === 'your_groq_api_key_here') {
      throw new Error('GROQ_API_KEY is required for summarization feature. Please add it to your .env file. Get a free key at: https://console.groq.com');
    }

    // Use our Groq utility for summarization
    // If no query provided, use a generic context
    const searchQuery = query || 'test cases';
    
    const summary = await summarizeResults(searchQuery, results, {
      style: summaryType === 'detailed' ? 'detailed' : 'concise',
      maxLength: summaryType === 'detailed' ? 1000 : 300,
      includeMetrics: summaryType === 'detailed'
    });

    console.log('âœ… Summarization complete with Groq AI');

    res.json({
      success: true,
      summary: summary,
      summaryType,
      resultCount: results.length,
      model: process.env.GROQ_SUMMARIZATION_MODEL || 'llama-3.3-70b-versatile',
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('Summarization error:', error);
    res.status(500).json({ 
      error: 'Summarization failed', 
      details: error.message 
    });
  }
});

// Old TestLeaf implementation removed - now using Groq AI for summarization
// See groqClient.js utility for the new implementation

// ======================== Test Prompt Endpoint ========================
app.post('/api/test-prompt', async (req, res) => {
  try {
    const { prompt, temperature = 0.2, maxTokens = 1000 } = req.body;
    
    if (!prompt) {
      return res.status(400).json({ error: 'Prompt is required' });
    }

    // Use Groq AI for chat completion
    console.log('ðŸ¤– Using Groq AI for prompt testing...');
    
    if (!process.env.GROQ_API_KEY || process.env.GROQ_API_KEY === 'your_groq_api_key_here') {
      throw new Error('GROQ_API_KEY is required for prompt testing. Please add it to your .env file.');
    }

    // Import Groq client dynamically
    const Groq = (await import('groq-sdk')).default;
    const groq = new Groq({
      apiKey: process.env.GROQ_API_KEY
    });

    const completion = await groq.chat.completions.create({
      messages: [
        { role: 'user', content: prompt }
      ],
      model: process.env.GROQ_RERANK_MODEL || 'llama-3.2-3b-preview',
      temperature: temperature,
      max_tokens: Math.min(maxTokens, 2000)
    });

    const aiResponse = completion.choices[0].message.content;
    const usage = completion.usage;

    console.log('âœ… Groq response received');

    // Calculate estimated cost (Groq pricing: $0.05/M input, $0.10/M output)
    const inputCost = (usage.prompt_tokens / 1000000) * 0.05;
    const outputCost = (usage.completion_tokens / 1000000) * 0.10;
    const totalCost = inputCost + outputCost;

    // Try to parse as JSON
    let parsedResponse;
    try {
      parsedResponse = JSON.parse(aiResponse);
    } catch (e) {
      parsedResponse = { raw: aiResponse };
    }

    return res.json({
      success: true,
      response: parsedResponse,
      model: process.env.GROQ_RERANK_MODEL || 'llama-3.2-3b-preview',
      provider: 'groq-ai',
      tokens: {
        prompt: usage.prompt_tokens,
        completion: usage.completion_tokens,
        total: usage.total_tokens
      },
      cost: {
        input: inputCost.toFixed(6),
        output: outputCost.toFixed(6),
        total: totalCost.toFixed(6)
      },
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('Prompt test error:', error);
    return res.status(500).json({ 
      error: 'Failed to test prompt', 
      details: error.message
    });
  }
});

// Helper function to calculate text similarity (Jaccard similarity)
function calculateTextSimilarity(text1, text2) {
  const words1 = new Set(text1.toLowerCase().split(/\s+/));
  const words2 = new Set(text2.toLowerCase().split(/\s+/));
  
  const intersection = new Set([...words1].filter(x => words2.has(x)));
  const union = new Set([...words1, ...words2]);
  
  return intersection.size / union.size;
}

// Search vector database
app.post('/api/search', async (req, res) => {
  try {
    const { query, limit = 5, filters = {} } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    // Create a MongoClient, connect once, validate DB/collection/index and reuse for the search
    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });
    await mongoClient.connect();
    // Validate DB/collection/index and ensure documents exist
    const validation = await validateDbCollectionIndex(mongoClient, process.env.DB_NAME, process.env.COLLECTION_NAME, process.env.VECTOR_INDEX_NAME, true);
    if (!validation.ok) {
      try { await mongoClient.close(); } catch (e) {}
      return res.status(400).json({ error: validation.error });
    }

    const db = mongoClient.db(process.env.DB_NAME);
    const collection = db.collection(process.env.COLLECTION_NAME);

    // Generate embedding for query using Mistral AI
    console.log('ðŸ”„ Generating embedding with Mistral AI...');
    const embeddingResult = await generateEmbedding(query);
    
    if (!embeddingResult || !embeddingResult.embedding) {
      throw new Error('Failed to generate embedding with Mistral AI');
    }

    const queryVector = embeddingResult.embedding;
    const tokens = embeddingResult.usage?.total_tokens || 0;
    const cost = (tokens / 1000000) * 0.10; // Mistral pricing: $0.10 per 1M tokens
    
    console.log(`âœ… Embedding generated! Cost: $${cost.toFixed(6)}, Tokens: ${tokens}`);

    // Calculate candidates and internal limit for vector search
    const requestedLimit = parseInt(limit);
    const numCandidates = Math.max(100, requestedLimit * 10); // At least 100 candidates
    const vectorSearchLimit = Math.min(numCandidates, requestedLimit * 10); // Limit must be <= numCandidates

    // Build vector search WITHOUT pre-filtering (to avoid index requirement)
    const vectorSearchStage = {
      $vectorSearch: {
        queryVector,
        path: "embedding",
        numCandidates: numCandidates,
        limit: vectorSearchLimit, // Get more candidates for post-filtering
        index: process.env.VECTOR_INDEX_NAME
      }
    };

    // Build the pipeline
    const pipeline = [
      vectorSearchStage,
      {
        $addFields: {
          score: { $meta: "vectorSearchScore" }
        }
      }
    ];

    // Apply metadata filters using $match stage (works without index)
    if (Object.keys(filters).length > 0) {
      const matchConditions = {};
      Object.entries(filters).forEach(([key, value]) => {
        if (value) {
          matchConditions[key] = value;
        }
      });
      pipeline.push({
        $match: matchConditions
      });
      console.log('ðŸ” Applying filters with $match:', matchConditions);
    }

    // Add limit after filtering
    pipeline.push({
      $limit: requestedLimit
    });

    // Project fields
    pipeline.push({
      $project: {
        id: 1,
        module: 1,
        preRequisites: 1,
        title: 1,
        description: 1,
        steps: 1,
        expectedResults: 1,
        automationManual: 1,
        priority: 1,
        createdBy: 1,
        createdDate: 1,
        lastModifiedDate: 1,
        risk: 1,
        version: 1,
        type: 1,
        sourceFile: 1,
        createdAt: 1,
        score: 1
      }
    });

    console.log('ðŸ” Search Query:', query);
    console.log('ðŸ” Filters:', JSON.stringify(filters));
    console.log('ðŸ” Pipeline:', JSON.stringify(pipeline, null, 2));

    const results = await collection.aggregate(pipeline).toArray();
    console.log('âœ… Found results:', results.length);
    
    await mongoClient.close();

    const responseData = {
      success: true,
      query,
      filters,
      results,
      cost: cost,
      tokens: tokens,
      model: 'mistral-embed'
    };
    
    console.log('ðŸ“¤ Sending response with', results.length, 'results');
    res.json(responseData);

  } catch (error) {
    console.error('âŒ Search failed:', error.message);
    console.error('Error details:', error);
    res.status(500).json({ error: 'Search failed', details: error.message });
  }
});

// ======================== BM25 Search Endpoint ========================
app.post('/api/search/bm25', async (req, res) => {
  try {
    const { query, limit = 10, filters = {}, fields = ['title', 'description', 'steps', 'expectedResults', 'module'] } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    console.log(`ðŸ”¤ BM25 Search request: "${query}"`);
    console.log(`   Limit: ${limit}`);
    console.log(`   Filters:`, filters);

    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });

    await mongoClient.connect();

    const validation = await validateDbCollectionIndex(
      mongoClient, 
      process.env.DB_NAME, 
      process.env.COLLECTION_NAME, 
      process.env.BM25_INDEX_NAME,
      true
    );
    
    if (!validation.ok) {
      try { await mongoClient.close(); } catch (e) {}
      return res.status(400).json({ error: validation.error });
    }

    const db = mongoClient.db(process.env.DB_NAME);
    const collection = db.collection(process.env.COLLECTION_NAME);

    // Build BM25 search pipeline
    const pipeline = [
      {
        $search: {
          index: process.env.BM25_INDEX_NAME,
          text: {
            query: query,
            path: fields,
            fuzzy: {
              maxEdits: 1,
              prefixLength: 2
            }
          }
        }
      },
      {
        $addFields: {
          score: { $meta: "searchScore" }
        }
      }
    ];

    // Apply filters if provided
    if (Object.keys(filters).length > 0) {
      const matchConditions = {};
      Object.entries(filters).forEach(([key, value]) => {
        if (value && value !== '') {
          matchConditions[key] = value;
        }
      });

      if (Object.keys(matchConditions).length > 0) {
        pipeline.push({ $match: matchConditions });
      }
    }

    // Add projection and limit
    pipeline.push(
      {
        $project: {
          id: 1,
          module: 1,
          title: 1,
          description: 1,
          steps: 1,
          expectedResults: 1,
          priority: 1,
          risk: 1,
          automationManual: 1,
          sourceFile: 1,
          createdAt: 1,
          score: 1
        }
      },
      { $limit: parseInt(limit) }
    );

    console.log('ðŸ” BM25 Pipeline:', JSON.stringify(pipeline, null, 2));

    const startTime = Date.now();
    const results = await collection.aggregate(pipeline).toArray();
    const searchTime = Date.now() - startTime;

    await mongoClient.close();

    console.log(`âœ… BM25 Search complete: ${results.length} results in ${searchTime}ms`);

    res.json({
      success: true,
      searchType: 'bm25',
      query,
      filters,
      results,
      count: results.length,
      searchTime,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('âŒ BM25 Search error:', error);
    res.status(500).json({ 
      error: 'BM25 search failed', 
      details: error.message 
    });
  }
});

// ======================== Hybrid Search Endpoint (BM25 + Vector) ========================
app.post('/api/search/hybrid', async (req, res) => {
  try {
    const { 
      query, 
      limit = 10, 
      filters = {},
      bm25Weight = 0.5,
      vectorWeight = 0.5,
      bm25Fields = ['title', 'description', 'steps', 'expectedResults', 'module']
    } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    console.log(`ðŸ”€ Hybrid Search request: "${query}"`);
    console.log(`   BM25 Weight: ${bm25Weight}, Vector Weight: ${vectorWeight}`);

    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });

    await mongoClient.connect();

    // Validate both indexes exist
    const bm25Validation = await validateDbCollectionIndex(
      mongoClient, 
      process.env.DB_NAME, 
      process.env.COLLECTION_NAME, 
      process.env.BM25_INDEX_NAME,
      true
    );
    
    const vectorValidation = await validateDbCollectionIndex(
      mongoClient, 
      process.env.DB_NAME, 
      process.env.COLLECTION_NAME, 
      process.env.VECTOR_INDEX_NAME,
      true
    );

    if (!bm25Validation.ok) {
      await mongoClient.close();
      return res.status(400).json({ error: `BM25 Index: ${bm25Validation.error}` });
    }

    if (!vectorValidation.ok) {
      await mongoClient.close();
      return res.status(400).json({ error: `Vector Index: ${vectorValidation.error}` });
    }

    const db = mongoClient.db(process.env.DB_NAME);
    const collection = db.collection(process.env.COLLECTION_NAME);

    const searchLimit = parseInt(limit) * 3; // Get more for better combination

    // 1. BM25 Search
    console.log('ðŸ”¤ Running BM25 search...');
    const bm25StartTime = Date.now();
    
    const bm25Pipeline = [
      {
        $search: {
          index: process.env.BM25_INDEX_NAME,
          text: {
            query: query,
            path: bm25Fields,
            fuzzy: {
              maxEdits: 1,
              prefixLength: 2
            }
          }
        }
      },
      {
        $addFields: {
          bm25Score: { $meta: "searchScore" }
        }
      },
      {
        $project: {
          _id: 1,
          id: 1,
          module: 1,
          title: 1,
          description: 1,
          steps: 1,
          expectedResults: 1,
          priority: 1,
          risk: 1,
          automationManual: 1,
          sourceFile: 1,
          createdAt: 1,
          bm25Score: 1
        }
      },
      { $limit: searchLimit }
    ];

    const bm25Results = await collection.aggregate(bm25Pipeline).toArray();
    const bm25Time = Date.now() - bm25StartTime;

    // 2. Vector Search with Mistral AI
    console.log('ðŸ§  Running vector search with Mistral AI...');
    const vectorStartTime = Date.now();

    const embeddingResult = await generateEmbedding(query);
    
    if (!embeddingResult || !embeddingResult.embedding) {
      throw new Error('Failed to generate embedding with Mistral AI');
    }

    const queryVector = embeddingResult.embedding;
    const tokens = embeddingResult.usage?.total_tokens || 0;
    const embeddingCost = (tokens / 1000000) * 0.10;

    const vectorPipeline = [
      {
        $vectorSearch: {
          queryVector,
          path: "embedding",
          numCandidates: 100,
          limit: searchLimit,
          index: process.env.VECTOR_INDEX_NAME
        }
      },
      {
        $addFields: {
          vectorScore: { $meta: "vectorSearchScore" }
        }
      },
      {
        $project: {
          _id: 1,
          id: 1,
          module: 1,
          title: 1,
          description: 1,
          steps: 1,
          expectedResults: 1,
          priority: 1,
          risk: 1,
          automationManual: 1,
          sourceFile: 1,
          createdAt: 1,
          vectorScore: 1
        }
      }
    ];

    const vectorResults = await collection.aggregate(vectorPipeline).toArray();
    const vectorTime = Date.now() - vectorStartTime;

    // 3. Normalize and combine scores
    console.log('ðŸ”€ Combining results...');
    
    // Normalize BM25 scores
    const bm25Scores = bm25Results.map(r => r.bm25Score);
    const bm25Max = Math.max(...bm25Scores, 1);
    const bm25Min = Math.min(...bm25Scores, 0);
    const bm25Range = bm25Max - bm25Min || 1;

    // Normalize Vector scores
    const vectorScores = vectorResults.map(r => r.vectorScore);
    const vectorMax = Math.max(...vectorScores, 1);
    const vectorMin = Math.min(...vectorScores, 0);
    const vectorRange = vectorMax - vectorMin || 1;

    // Create result map
    const resultMap = new Map();

    // Add BM25 results with normalized scores
    bm25Results.forEach(result => {
      const key = result._id.toString();
      const normalizedScore = (result.bm25Score - bm25Min) / bm25Range;
      resultMap.set(key, {
        ...result,
        bm25ScoreNormalized: normalizedScore,
        vectorScore: 0,
        vectorScoreNormalized: 0,
        hybridScore: normalizedScore * bm25Weight,
        foundIn: 'bm25'
      });
    });

    // Add/merge vector results with normalized scores
    vectorResults.forEach(result => {
      const key = result._id.toString();
      const normalizedScore = (result.vectorScore - vectorMin) / vectorRange;
      
      if (resultMap.has(key)) {
        // Merge - found in both
        const existing = resultMap.get(key);
        existing.vectorScore = result.vectorScore;
        existing.vectorScoreNormalized = normalizedScore;
        existing.hybridScore += normalizedScore * vectorWeight;
        existing.foundIn = 'both';
      } else {
        // New result - only in vector
        resultMap.set(key, {
          ...result,
          bm25Score: 0,
          bm25ScoreNormalized: 0,
          vectorScoreNormalized: normalizedScore,
          hybridScore: normalizedScore * vectorWeight,
          foundIn: 'vector'
        });
      }
    });

    // Convert to array and sort by hybrid score
    let combinedResults = Array.from(resultMap.values());
    combinedResults.sort((a, b) => b.hybridScore - a.hybridScore);

    // Apply filters if provided
    if (Object.keys(filters).length > 0) {
      combinedResults = combinedResults.filter(result => {
        return Object.entries(filters).every(([key, value]) => {
          if (!value || value === '') return true;
          return result[key] === value;
        });
      });
    }

    // Limit results
    const finalResults = combinedResults.slice(0, parseInt(limit));

    await mongoClient.close();

    const totalTime = Date.now() - bm25StartTime;
    console.log(`âœ… Hybrid Search complete: ${finalResults.length} results in ${totalTime}ms`);

    // Calculate statistics
    const bothCount = finalResults.filter(r => r.foundIn === 'both').length;
    const bm25OnlyCount = finalResults.filter(r => r.foundIn === 'bm25').length;
    const vectorOnlyCount = finalResults.filter(r => r.foundIn === 'vector').length;

    res.json({
      success: true,
      searchType: 'hybrid',
      query,
      filters,
      weights: { bm25: bm25Weight, vector: vectorWeight },
      results: finalResults,
      count: finalResults.length,
      stats: {
        foundInBoth: bothCount,
        foundInBm25Only: bm25OnlyCount,
        foundInVectorOnly: vectorOnlyCount,
        bm25ResultCount: bm25Results.length,
        vectorResultCount: vectorResults.length
      },
      timing: {
        bm25Time,
        vectorTime,
        totalTime
      },
      cost: embeddingCost,
      tokens: tokens,
      model: 'mistral-embed',
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('âŒ Hybrid Search error:', error);
    res.status(500).json({ 
      error: 'Hybrid search failed', 
      details: error.message 
    });
  }
});

// Reranking endpoint with Score Fusion and Normalization
app.post('/api/search/rerank', async (req, res) => {
  try {
    const { 
      query, 
      limit = 10, 
      filters = {}, 
      fusionMethod = 'rrf', // rrf, weighted, or reciprocal
      rerankTopK = 50,
      bm25Weight = 0.4,
      vectorWeight = 0.6
    } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query is required' });
    }

    const startTime = Date.now();

    // Create a MongoClient
    const mongoClient = new MongoClient(process.env.MONGODB_URI, {
      ssl: true,
      tlsAllowInvalidCertificates: true,
      tlsAllowInvalidHostnames: true,
      serverSelectionTimeoutMS: 30000,
      connectTimeoutMS: 30000,
      socketTimeoutMS: 30000,
    });
    await mongoClient.connect();

    const db = mongoClient.db(process.env.DB_NAME);
    const collection = db.collection(process.env.COLLECTION_NAME);

    console.log(`\nðŸ”„ Reranking Search with Score Fusion for: "${query}"`);
    console.log(`ðŸ“Š Fusion Method: ${fusionMethod.toUpperCase()}, Top-K: ${rerankTopK}, Final Limit: ${limit}`);

    // Step 1: Generate embedding with Mistral AI
    console.log('ðŸ”„ Generating embedding with Mistral AI...');
    const embeddingResult = await generateEmbedding(query);
    
    if (!embeddingResult || !embeddingResult.embedding) {
      throw new Error('Failed to generate embedding with Mistral AI');
    }

    const queryVector = embeddingResult.embedding;
    const embeddingTokens = embeddingResult.usage?.total_tokens || 0;
    const embeddingCost = (embeddingTokens / 1000000) * 0.10;
    
    console.log(`âœ… Embedding generated! Cost: $${embeddingCost.toFixed(6)}, Tokens: ${embeddingTokens}`);

    // Parallel search: BM25 and Vector
    const searchStartTime = Date.now();

    // BM25 Pipeline
    const weights = {
      id: 10.0,
      title: 8.0,
      module: 5.0,
      description: 2.0,
      expectedResults: 1.5,
      steps: 1.0,
      preRequisites: 0.8
    };

    const searchFields = Object.entries(weights).map(([field, weight]) => ({
      text: {
        query: query,
        path: field,
        fuzzy: { maxEdits: 1, prefixLength: 2 },
        score: { boost: { value: weight } }
      }
    }));

    const bm25Pipeline = [
      {
        $search: {
          index: process.env.BM25_INDEX_NAME,
          compound: {
            should: searchFields,
            minimumShouldMatch: 1
          }
        }
      },
      {
        $addFields: {
          bm25Score: { $meta: "searchScore" }
        }
      },
      { $limit: rerankTopK }
    ];

    if (Object.keys(filters).length > 0) {
      bm25Pipeline.push({ $match: filters });
    }

    // Vector Pipeline
    const vectorPipeline = [
      {
        $vectorSearch: {
          queryVector,
          path: "embedding",
          numCandidates: Math.max(rerankTopK * 2, 100),
          limit: rerankTopK,
          index: process.env.VECTOR_INDEX_NAME,
          ...(Object.keys(filters).length > 0 && { filter: filters })
        }
      },
      {
        $addFields: {
          vectorScore: { $meta: "vectorSearchScore" }
        }
      },
      { $project: { embedding: 0 } }
    ];

    // Execute both searches in parallel
    const [bm25Results, vectorResults] = await Promise.all([
      collection.aggregate(bm25Pipeline).toArray(),
      collection.aggregate(vectorPipeline).toArray()
    ]);

    const searchTime = Date.now() - searchStartTime;
    console.log(`âœ… Retrieved ${bm25Results.length} BM25 + ${vectorResults.length} Vector results in ${searchTime}ms`);

    // Step 2: Score Fusion and Normalization
    const rerankStartTime = Date.now();
    console.log(`ðŸ”„ Applying ${fusionMethod.toUpperCase()} score fusion...`);

    // Create a map to combine results
    const resultMap = new Map();

    // Normalize scores using min-max normalization
    const normalizeBM25 = (score, minScore, maxScore) => {
      if (maxScore === minScore) return 1.0;
      return (score - minScore) / (maxScore - minScore);
    };

    const normalizeVector = (score, minScore, maxScore) => {
      if (maxScore === minScore) return 1.0;
      return (score - minScore) / (maxScore - minScore);
    };

    // Get min/max scores for normalization
    const bm25Scores = bm25Results.map(r => r.bm25Score);
    const vectorScores = vectorResults.map(r => r.vectorScore);
    const minBM25 = Math.min(...bm25Scores, 0);
    const maxBM25 = Math.max(...bm25Scores, 1);
    const minVector = Math.min(...vectorScores, 0);
    const maxVector = Math.max(...vectorScores, 1);

    // Process BM25 results
    bm25Results.forEach((doc, index) => {
      const id = doc._id.toString();
      const normalizedScore = normalizeBM25(doc.bm25Score, minBM25, maxBM25);
      
      resultMap.set(id, {
        ...doc,
        bm25Score: doc.bm25Score,
        bm25Normalized: normalizedScore,
        bm25Rank: index + 1,
        vectorScore: 0,
        vectorNormalized: 0,
        vectorRank: null,
        foundIn: 'bm25'
      });
    });

    // Process Vector results and merge
    vectorResults.forEach((doc, index) => {
      const id = doc._id.toString();
      const normalizedScore = normalizeVector(doc.vectorScore, minVector, maxVector);
      
      if (resultMap.has(id)) {
        // Document found in both
        const existing = resultMap.get(id);
        existing.vectorScore = doc.vectorScore;
        existing.vectorNormalized = normalizedScore;
        existing.vectorRank = index + 1;
        existing.foundIn = 'both';
      } else {
        // Document only in vector
        resultMap.set(id, {
          ...doc,
          bm25Score: 0,
          bm25Normalized: 0,
          bm25Rank: null,
          vectorScore: doc.vectorScore,
          vectorNormalized: normalizedScore,
          vectorRank: index + 1,
          foundIn: 'vector'
        });
      }
    });

    // Convert to array for processing
    const allResults = Array.from(resultMap.values());

    // Apply fusion method
    let fusedResults = [];

    if (fusionMethod === 'rrf') {
      // Reciprocal Rank Fusion (RRF)
      const k = 60; // RRF constant
      fusedResults = allResults.map(doc => {
        const bm25RRF = doc.bm25Rank ? 1 / (k + doc.bm25Rank) : 0;
        const vectorRRF = doc.vectorRank ? 1 / (k + doc.vectorRank) : 0;
        const fusedScore = bm25RRF + vectorRRF;
        
        return {
          ...doc,
          fusedScore,
          fusionComponents: {
            bm25RRF: bm25RRF.toFixed(4),
            vectorRRF: vectorRRF.toFixed(4)
          }
        };
      });
    } else if (fusionMethod === 'weighted') {
      // Weighted normalized scores
      fusedResults = allResults.map(doc => {
        const fusedScore = (doc.bm25Normalized * bm25Weight) + (doc.vectorNormalized * vectorWeight);
        
        return {
          ...doc,
          fusedScore,
          fusionComponents: {
            bm25Contribution: (doc.bm25Normalized * bm25Weight).toFixed(4),
            vectorContribution: (doc.vectorNormalized * vectorWeight).toFixed(4)
          }
        };
      });
    } else if (fusionMethod === 'reciprocal') {
      // Reciprocal scoring with weights
      fusedResults = allResults.map(doc => {
        const bm25Reciprocal = doc.bm25Rank ? (1 / doc.bm25Rank) * bm25Weight : 0;
        const vectorReciprocal = doc.vectorRank ? (1 / doc.vectorRank) * vectorWeight : 0;
        const fusedScore = bm25Reciprocal + vectorReciprocal;
        
        return {
          ...doc,
          fusedScore,
          fusionComponents: {
            bm25Reciprocal: bm25Reciprocal.toFixed(4),
            vectorReciprocal: vectorReciprocal.toFixed(4)
          }
        };
      });
    }

    // Sort by fused score
    fusedResults.sort((a, b) => b.fusedScore - a.fusedScore);

    // Add ranking information
    fusedResults.forEach((doc, index) => {
      doc.newRank = index + 1;
      doc.originalRank = doc.bm25Rank || doc.vectorRank || index + 1;
      doc.rankChange = doc.originalRank - doc.newRank;
    });

    const rerankingTime = Date.now() - rerankStartTime;

    // Get before results (before fusion)
    const beforeFusionResults = (fusionMethod === 'rrf' ? vectorResults : bm25Results).slice(0, limit);
    
    // Step 3: Optional Groq AI Reranking (after fusion)
    let finalResults = fusedResults.slice(0, limit);
    let groqRerankTime = 0;
    let usedGroqRerank = false;
    
    // Only use Groq reranking if GROQ_API_KEY is configured and not set to placeholder
    if (process.env.GROQ_API_KEY && 
        process.env.GROQ_API_KEY !== 'your_groq_api_key_here' && 
        fusedResults.length > 0) {
      try {
        console.log(`ðŸ¤– Applying Groq AI reranking to top ${rerankTopK} results...`);
        const groqStartTime = Date.now();
        
        const groqReranked = await rerankDocuments(query, fusedResults.slice(0, rerankTopK), limit);
        
        if (groqReranked && groqReranked.length > 0) {
          finalResults = groqReranked;
          usedGroqRerank = true;
          groqRerankTime = Date.now() - groqStartTime;
          console.log(`âœ… Groq reranking complete in ${groqRerankTime}ms`);
        }
      } catch (groqError) {
        console.warn(`âš ï¸  Groq reranking failed, using fusion results: ${groqError.message}`);
        // Fall back to fusion results if Groq fails
      }
    }
    
    const totalTime = Date.now() - startTime;

    console.log(`âœ… Score fusion complete in ${rerankingTime}ms`);
    console.log(`ðŸ“Š Top result: ${finalResults[0]?.id} (Found in: ${finalResults[0]?.foundIn})`);

    await mongoClient.close();

    // Calculate statistics
    const bothCount = fusedResults.filter(r => r.foundIn === 'both').length;
    const bm25OnlyCount = fusedResults.filter(r => r.foundIn === 'bm25').length;
    const vectorOnlyCount = fusedResults.filter(r => r.foundIn === 'vector').length;

    res.json({
      success: true,
      fusionMethod,
      query,
      filters,
      results: finalResults,
      beforeReranking: beforeFusionResults || [],  // Client expects this field
      afterReranking: finalResults || [],          // Client expects this field
      beforeFusion: beforeFusionResults || [],
      afterFusion: fusedResults.slice(0, limit) || [],
      finalResults: finalResults || [],
      reranked: true,
      usedGroqRerank,
      count: finalResults.length,
      totalCandidates: fusedResults.length,
      rerankTopK,
      searchTime,
      fusionTime: rerankingTime,
      groqRerankTime,
      totalTime,
      cost: embeddingCost,
      tokens: embeddingTokens,
      model: 'mistral-embed',
      rerankModel: usedGroqRerank ? (process.env.GROQ_RERANK_MODEL || 'groq-ai') : 'score-fusion',
      weights: { bm25: bm25Weight, vector: vectorWeight },
      stats: {
        foundInBoth: bothCount,
        foundInBm25Only: bm25OnlyCount,
        foundInVectorOnly: vectorOnlyCount,
        topResultChanged: beforeFusionResults[0]?.id !== finalResults[0]?.id,
        significantReorderings: finalResults.filter(r => Math.abs(r.rankChange || 0) >= 5).length,
        averageFusedScore: fusedResults.length > 0 ? (fusedResults.slice(0, limit).reduce((sum, r) => sum + r.fusedScore, 0) / Math.min(limit, fusedResults.length)).toFixed(4) : '0'
      },
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('âŒ Reranking error:', error);
    res.status(500).json({ 
      error: 'Reranking failed', 
      details: error.message 
    });
  }
});

// Start server
app.listen(PORT, () => {
  console.log(`ðŸš€ Server running on port ${PORT}`);
  console.log(`ðŸ“¡ API available at http://localhost:${PORT}/api`);
});